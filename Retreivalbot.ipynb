{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A SIMPLE RETRIEVAL BASED CHATBOT - LERATO\n",
    "A Retrieval based bot - using cosine similarity between words entered by the user and the words in the corpus.\n",
    "We 'll define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am so sorry! I dont understand your words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT NECCESSARY LIBRARIES OR DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # - Linear Algebra\n",
    "import random # - Random values/strings\n",
    "import nltk # - Natural languagua processing toolkit for Natural language preprocesing\n",
    "import string # - To process standard python strings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #convert a collection of raw text to a matrix of TF-IDF features\n",
    "from sklearn.metrics.pairwise import cosine_similarity #used to find the similarity between words/values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ TEXT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text =open('lerato.txt','r',errors = 'ignore') #Assign a variable to text path where the file is located\n",
    "Text =Text.read() #Read the path of the assigned variable and store in a new variable for preprocessing usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = Text.lower()# converts all text to lowercase; this help to avoid different meaning/pattern within text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(Text)# converts a text file to list of sentences \n",
    "word_tokens = nltk.word_tokenize(Text)# converts a text file to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zindi is the first data science competition platform in africa and hosts an entire data science ecosystem of scientists, engineers, academics, companies, ngos, governments and institutions focused on solving africa’s most pressing problems,\\n\\nfor data scientists, from newbies to rock stars, zindi is a place to access african datasets and solve african problems.',\n",
       " 'data scientists will find all the tools they need on zindi to compete, share ideas, hone their skills, build their professional profiles, find career opportunities, and have fun!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview tokenized sentence example\n",
    "sentence_tokens[:2] #Output the first 2 tokenized sentenced, you can tune to 1 and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zindi', 'is', 'the', 'first', 'data']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview tokenized word example\n",
    "word_tokens[:5] #Output the first 5 tokenized word, you can tune to see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "##WordNet is a semantically-oriented dictionary of English included in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a fxn to lemmatize the tokenized words\n",
    "def LemTokens(tokens):\n",
    "    lemmatized = [lemmer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store punctuations removal from words into a variable\n",
    "remove_punctuations = dict((ord(punctuations), None) for punctuations in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a fxn to Normalized lemmatized words i.e remove puctuations and convert all text to lower case \n",
    "def LemNormalize(text):\n",
    "    NormalizedLemmatized = LemTokens(nltk.word_tokenize(text.lower().translate(remove_punctuations)))\n",
    "    return  NormalizedLemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMPLE KEYWORD MATCHING\n",
    "Next,Define a fxn for a greeting by the AXABot i.e if a user’s input is a greeting, the bot respond with a greeting response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"what's up\",\"hey\")\n",
    "GREETING_RESPONSES = [\"i am here to attend to you\", \"hey\", \"how can i help you*\", \"what would you like to know about AXA Products/Services\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Response\n",
    "To generate a response from our bot for input questions, the concept of document similarity will be used. So functionality of the modules imported above will be utilizied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sentence_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am so sorry! I dont understand your words\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sentence_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon the user’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome: I am LERATO from Zindi. I will answer your queries about Zindi Platforms. If you want to exit the conversion, type Bye!\n",
      "hi\n",
      "LERATO: how can i help you*\n",
      "hello\n",
      "LERATO: hey\n",
      "what is zindi\n",
      "LERATO: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zindi is the first data science competition platform in africa and hosts an entire data science ecosystem of scientists, engineers, academics, companies, ngos, governments and institutions focused on solving africa’s most pressing problems,\n",
      "\n",
      "for data scientists, from newbies to rock stars, zindi is a place to access african datasets and solve african problems.\n",
      "how many numbers of submission per day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LERATO: maximum number of submissions are based on the challenge, kindly read instruction page\n",
      "how many numbers of team in a challenge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LERATO: maximum number of individual in a team are 4, to earn some points be among the top 10 in a price or reward challenge.\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "#welcome message\n",
    "print(\"Welcome: I am LERATO from Zindi. I will answer your queries about Zindi Platforms. If you want to exit the conversion, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input() #allows user input\n",
    "    user_response=user_response.lower() #convert user response to lower case for the botb\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' or user_response=='okay'):\n",
    "            flag=False\n",
    "            print(\"LERATO: You are welcome..\")\n",
    "        elif(greeting(user_response)!=None):\n",
    "            print(\"LERATO: \"+greeting(user_response))\n",
    "        else:\n",
    "            print(\"LERATO: \",end=\"\")\n",
    "            print(response(user_response))\n",
    "            sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"LERATO: Thanks, Stay active on the platform and help others to learn, Bye.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feel free to contibute and make it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
